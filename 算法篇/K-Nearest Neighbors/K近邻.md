# 简介

在机器学习模型中，参数模型利用训练数据求解出问题的一般性知识，再将这些知识通过模型加以外化。一旦模型的结构和参数被确定，他们就不再依赖训练数据，而是可以直接预测未知数据。

而在K近邻算法中，给定一个训练数据集，对于新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，**这K个实例的多数属于某个类**，就把该输入实例分类到这个类中。类似于生活中的少数服从多数。下面我们来看维基百科上的一张图

![aa ](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-c3f1d2553e7467d7da5f9cd538d2b49a_720w.png)

有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的绿色的圆所标示的数据则是**待分类的数据**。来了一个新的数据点，我要得到它的类别是什么？好的，下面我们根据k近邻的思想来给绿色圆点进行分类

* 如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，按照少数服从多数的原则，绿色圆点归于红色的三角形这一类。
* 如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，绿色圆点属于蓝色的正方形一类。

k均值的分类实质上是近邻区域内多个训练实例的平均。K越大近邻区域包含的点越多，平均化程度越高，就越容易避免噪声；K越小，区域越狭窄，平均化程度越低，这时的分类结果只 由离未知数据点最近的少量训练实例决定。



作为一种局部加权模型，如果将K直接等于N（训练样本的个数），则无论输入是什么，都会将它直接归为训练样本中最多的类,相当于直接统计出来圆形最多，将每个新的输入都归为圆形

![](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-e79d46a56c426061a9494091f8fac068_720w.png)

因此，**超参数k**既不能过大，也不能过小，一般我们选取一个较小的k值，通过**交叉验证**（按照一定比例，将样本数据分为训练用的数据和验证用的数据）的方法来选取最优的K值。通过计算验证集合的方差来最终找到一个比较合适的值

如下图

![](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\1011838-20190401220304846-2066630053.png)

一开始增加K时，错误率会先降低，因为样本更多了，但如果一直增加，错误率会更高，比如一共就30个样本，K增大到28时，KNN基本就没意义了。

## 衡量距离

k近邻算法是在训练数据集中找到与该实例**最邻近**的K个实例，如何定义**邻近**呢？

最常用的距离度量是**欧式距离**。

![](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-60bb382b0d22ec0ce296ed0e024f31bc_720w.png)

我们也常用欧式距离来衡量高维空间中两点的距离。

## 特征归一化

首先举例如下，我们用用一个人身高(cm)与脚码（尺码）大小来作为特征值，类别为男性或者女性。我们现在如果有5个训练样本，分布如下：

A [(179,42),男] B [(178,43),男] C [(165,36)女] D [(177,42),男] E [(160,35),女]

很容易看到第一维身高特征是第二维脚码特征的4倍左右，那么在进行距离度量的时候，**我们就会偏向于第一维特征。**这样造成两个特征并不是等价重要的，最终可能会导致距离计算错误，从而导致预测错误。

现在有一个测试样本 F(167,43)，让我们来预测他是男性还是女性，我们采取k=3来预测。

下面我们用欧式距离分别算出F离训练样本的欧式距离，然后选取最近的3个，多数类别就是我们最终的结果，计算如下：



![img](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-07d94c435dc95d66091768d56499f363_720w.png)

由计算可以得到，最近的前三个分别是C,D,E三个样本，那么由C,E为女性，D为男性，女性多于男性得到我们要预测的结果为**女性**。

这样问题就来了，一个女性的脚43码的可能性，远远小于男性脚43码的可能性，那么为什么算法还是会预测F为女性呢？那是因为由于各个特征量纲的不同，在这里导致了身高的重要性已经远远大于脚码了，这是不客观的，应该让每个特征都是同等重要吗，这也是我们要归一化的原因。

归一化公式如下：

![img](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-be30691d37ac93b2237217cadca2e967_720w.png)

## 实战

这个数据集叫iris，中文名是安德森鸢尾花卉数据集，iris包含150个样本，对应数据集的每行数据。每行数据包含每个样本的四个特征和样本的类别信息。

通俗地说，iris数据集是用来给花做分类的数据集，每个样本包含了花瓣长度等四个特征。一共有三种不同的鸢尾花——山鸢尾、变色鸢尾和维吉尼亚鸢尾。

对鸢尾花数据集进行分类的任务就是要根据鸢尾花的四个特征来判断这个样本属于哪一类鸢尾花。

具体过程见代码部分。

##  总结

作为典型的非参数方法， 近邻算法的运行方式和以线性回归为代表的参数方法截然相反。 线性回归的运算量主要花在参数拟合上，利用大量的训练数据来拟合出使均方误差最小的一 组参数。 一旦这组参数被计算出来，训练数据的历史使命就完成了，新来的数据都会用这组参数来处 理。而k近邻算法的训练过程没那么复杂，只需要将数据存储下来就行了。而对新实例进行分类时， 近邻算法需要找到离它最近的 个训练实例，这才是算法主要的运算负荷。
