# 简介

在机器学习模型中，参数模型利用训练数据求解出问题的一般性知识，再将这些知识通过模型加以外化。一旦模型的结构和参数被确定，他们就不再依赖训练数据，而是可以直接预测未知数据。

而在K近邻算法中，给定一个训练数据集，对于新的输入实例，在训练数据集中找到与该实例最邻近的JK个实例，**这K个实例的多数属于某个类**，就把该输入实例分类到这个类中。类似于生活中的少数服从多数。下面我们来看维基百科上的一张图

![aa ](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-c3f1d2553e7467d7da5f9cd538d2b49a_720w.png)

有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的绿色的圆所标示的数据则是**待分类的数据**。来了一个新的数据点，我要得到它的类别是什么？好的，下面我们根据k近邻的思想来给绿色圆点进行分类

* 如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，按照少数服从多数的原则，绿色圆点归于红色的三角形这一类。
* 如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，绿色圆点属于蓝色的正方形一类。

k均值的分类实质上是近邻区域内多个训练实例的平均。K越大近邻区域包含的点越多，平均化程度越高，就越容易避免噪声；K越小，区域越狭窄，平均化程度越低，这时的分类结果只 由离未知数据点最近的少量训练实例决定。



作为一种局部加权模型，如果将K直接等于N（训练样本的个数），则无论输入是什么，都会将它直接归为训练样本中最多的类,相当于直接统计出来圆形最多，将每个新的输入都归为圆形

![](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-e79d46a56c426061a9494091f8fac068_720w.png)

因此，**超参数k**既不能过大，也不能过小，一般我们选取一个较小的k值，通过**交叉验证**（按照一定比例，将样本数据分为训练用的数据和验证用的数据）的方法来选取最优的K值。通过计算验证集合的方差来最终找到一个比较合适的值

如下图

![](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\1011838-20190401220304846-2066630053.png)

一开始增加K时，错误率会先降低，因为样本更多了，但如果一直增加，错误率会更高，比如一共就30个样本，K增大到28时，KNN基本就没意义了。

## 衡量距离

k近邻算法是在训练数据集中找到与该实例**最邻近**的K个实例，如何定义**邻近**呢？

最常用的距离度量是**欧式距离**。

![](D:\GithubHere\rgzn-icu\算法篇\K-Nearest Neighbors\assets\v2-60bb382b0d22ec0ce296ed0e024f31bc_720w.png)

我们也常用欧式距离来衡量高维空间中两点的距离。

